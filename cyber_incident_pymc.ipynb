{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c2ef44",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Bayesian Cyber Risk Simulation\n",
    "\n",
    "This notebook provides a fully-documented, production-grade workflow for cyber loss quantification using a **Bayesian frequency/severity model** combined with a **Markov-chain attacker progression** through **MITRE ATT&CK** stages and **FAIR** loss taxonomy. It is designed for both analytical rigor and stakeholder communication.\n",
    "\n",
    "### What you will get\n",
    "- **Posterior inference** for frequency (Œª) and per-stage success probabilities\n",
    "- **Attacker behavior simulation** with retries, fallbacks, detection, and furthest-stage tracking\n",
    "- **FAIR-aligned losses** (lognormal bodies + Pareto tails for catastrophic events)\n",
    "- **Option B** support for observed incident counts (Poisson likelihood)\n",
    "- **Long-tail-aware visualizations** and **CSV exports** for downstream reporting\n",
    "\n",
    "> **Tip:** This is configured with a larger sampling sizes and may take a while. You can adjust settings in the next section if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc7eb6",
   "metadata": {},
   "source": [
    "\n",
    "## 1Ô∏è‚É£ Configuration and Imports (Production Defaults)\n",
    "\n",
    "This block imports all required libraries and defines global configuration. Key elements:\n",
    "- **Sampling settings** for PyMC (draws/tune/chains) are set for production-level fidelity.\n",
    "- **Monte Carlo** settings determine how many attacker attempts are simulated per posterior draw.\n",
    "- **Markov parameters** control retries, fallbacks, and detection dynamics per stage.\n",
    "- **Observed data** (`observed_total_incidents`, `observed_years`) are included but default to `None` so the model runs prior-driven until you populate them.\n",
    "- **Visualization** preference for plotting in USD or millions.\n",
    "\n",
    "If running in environments where multiprocessing is constrained (e.g., some Jupyter setups), keep `RUN_PARALLEL=False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58c02ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random, datetime, multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import Dict\n",
    "import numpy as np, pandas as pd, pymc as pm\n",
    "from scipy import stats, optimize\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Frequency prior (attempts/year), elicited as a 90% CI -> Lognormal params\n",
    "CI_MIN_FREQ = 1\n",
    "CI_MAX_FREQ = 30\n",
    "Z_90 = 1.645\n",
    "\n",
    "# PyMC sampling (tune for speed vs. accuracy)\n",
    "N_SAMPLES = 1500\n",
    "N_TUNE = 1500\n",
    "N_CHAINS = 4\n",
    "TARGET_ACCEPT = 0.90 \n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# ---- Monte Carlo attempts per posterior draw ----\n",
    "RUN_PARALLEL = False       # Set True only if your notebook environment supports multiprocessing\n",
    "N_WORKERS = None           # None -> use cpu_count()\n",
    "N_SIM_PER_DRAW = 500       # Per-draw attempts to estimate per-attempt success\n",
    "\n",
    "# ---- Markov behavior for attacker progression ----\n",
    "MAX_RETRIES_PER_STAGE = 3\n",
    "RETRY_PENALTY = 0.75       # geometric penalty to success prob on each retry\n",
    "FALLBACK_PROB = 0.25       # chance to back up and try previous stage after exhausting retries\n",
    "DETECT_BASE = 0.01         # baseline chance of detection on a failed try\n",
    "DETECT_INC_PER_RETRY = 0.06# detection increase with each retry\n",
    "\n",
    "# ---- Visualization ----\n",
    "PLOT_IN_MILLIONS = True\n",
    "\n",
    "# ---- Option B: observed incident counts over an exposure window (optional) ----\n",
    "# Set both integers to condition posterior; leave as None to skip.\n",
    "observed_total_incidents = None   # e.g., 7 if 7 successful incidents observed\n",
    "observed_years = None             # e.g., 3 if those occurred over 3 years\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaba695",
   "metadata": {},
   "source": [
    "\n",
    "## 2Ô∏è‚É£ Prior Construction: MITRE ATT&CK Stage Success and FAIR Loss Bodies/Tails\n",
    "\n",
    "**Goal:** Convert SME/control-effectiveness uncertainty into usable statistical priors.\n",
    "\n",
    "- We map each **MITRE ATT&CK** stage to a **control-effectiveness 90% CI**. Since the model needs attacker *success* probabilities per stage, we invert those ranges and fit **Beta(Œ±,Œ≤)** using quantile matching (5% and 95% points).\n",
    "- For **FAIR** categories, we parameterize **lognormal** bodies from 5%/95% quantiles and add **Pareto** tails (for rare catastrophic events) for **Regulatory/Legal** and **Reputation/Competitive**.\n",
    "\n",
    "> This separation lets you later plug in organization-specific priors without changing the rest of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SME-provided 90% CI for control effectiveness per MITRE stage.\n",
    "# We convert to attacker success: success ‚àà [1 - hi, 1 - lo], then fit Beta via quantile-matching.\n",
    "STAGE_CONTROL_MAP = {\n",
    "    \"Initial Access\":        (0.15, 0.50),\n",
    "    \"Execution\":             (0.05, 0.30),\n",
    "    \"Persistence\":           (0.10, 0.45),\n",
    "    \"Privilege Escalation\":  (0.10, 0.35),\n",
    "    \"Defense Evasion\":       (0.20, 0.60),\n",
    "    \"Credential Access\":     (0.15, 0.50),\n",
    "    \"Discovery\":             (0.02, 0.20),\n",
    "    \"Lateral Movement\":      (0.25, 0.65),\n",
    "    \"Collection\":            (0.05, 0.30),\n",
    "    \"Command and Control\":   (0.10, 0.45),\n",
    "    \"Exfiltration\":          (0.30, 0.70),\n",
    "    \"Impact\":                (0.35, 0.75),\n",
    "}\n",
    "MITRE_STAGES = list(STAGE_CONTROL_MAP.keys())\n",
    "\n",
    "def _quantile_match_beta(p5, p95, q_low=0.05, q_high=0.95):\n",
    "    mean = 0.5*(p5+p95)\n",
    "    width = max(p95-p5, 1e-6)\n",
    "    conc_guess = 20.0 * (0.1/width)\n",
    "    a0 = max(1e-3, mean*conc_guess)\n",
    "    b0 = max(1e-3, (1-mean)*conc_guess)\n",
    "    def resid(params):\n",
    "        a,b = params\n",
    "        if a<=0 or b<=0: return [1e6,1e6]\n",
    "        return [stats.beta.ppf(q_low,a,b)-p5, stats.beta.ppf(q_high,a,b)-p95]\n",
    "    sol = optimize.root(resid, [a0,b0], method=\"hybr\")\n",
    "    if sol.success and np.all(np.array(sol.x) > 0): return float(sol.x[0]), float(sol.x[1])\n",
    "    return a0,b0\n",
    "\n",
    "_success_90s = [(1.0 - hi, 1.0 - lo) for lo, hi in STAGE_CONTROL_MAP.values()]\n",
    "alphas, betas = zip(*(_quantile_match_beta(lo,hi) for lo,hi in _success_90s))\n",
    "alphas, betas = np.array(alphas), np.array(betas)\n",
    "\n",
    "# --- FAIR taxonomy (bodies via lognormal, tails via Pareto) ---\n",
    "loss_categories = [\"Productivity\",\"ResponseContainment\",\"RegulatoryLegal\",\"ReputationCompetitive\"]\n",
    "loss_q5_q95 = {\n",
    "    \"Productivity\": (10_000, 150_000),\n",
    "    \"ResponseContainment\": (20_000, 500_000),\n",
    "    \"RegulatoryLegal\": (0, 1_000_000),\n",
    "    \"ReputationCompetitive\": (0, 2_000_000),\n",
    "}\n",
    "def _lognormal_from_q5_q95(q5,q95):\n",
    "    q5, q95 = max(q5,1.0), max(q95, q5*1.0001)\n",
    "    ln5, ln95 = np.log(q5), np.log(q95)\n",
    "    sigma = (ln95 - ln5) / (2.0 * Z_90)\n",
    "    mu = 0.5 * (ln5 + ln95)\n",
    "    return mu, sigma\n",
    "cat_mu = np.zeros(len(loss_categories)); cat_sigma = np.zeros(len(loss_categories))\n",
    "for i,cat in enumerate(loss_categories):\n",
    "    cat_mu[i], cat_sigma[i] = _lognormal_from_q5_q95(*loss_q5_q95[cat])\n",
    "pareto_defaults = {\n",
    "    \"RegulatoryLegal\": {\"xm\": 100_000.0, \"alpha\": 1.8},\n",
    "    \"ReputationCompetitive\": {\"xm\": 250_000.0, \"alpha\": 1.5}\n",
    "}\n",
    "print(\"MITRE priors and FAIR loss parameters configured.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299e5b2",
   "metadata": {},
   "source": [
    "\n",
    "## 3Ô∏è‚É£ Bayesian Model & Sampling (Œª and Stage Success)\n",
    "\n",
    "This section builds and samples a **PyMC** model that estimates:\n",
    "- **Œª** (annual attack attempt frequency), modeled as a **Lognormal** with a 90% CI prior.\n",
    "- **Stage success probabilities** across MITRE ATT&CK, modeled as independent **Betas** derived above.\n",
    "\n",
    "It also adds an **optional observed-data likelihood** (Option B):\n",
    "- If you provide `observed_total_incidents` and `observed_years`, the model includes `Poisson(Œª √ó years)` and conditions on that evidence.\n",
    "- If either is `None`, the model runs **prior-only**.\n",
    "\n",
    "We then sample using **NUTS** with production-level settings defined earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb1889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pm.Model() as model:\n",
    "    mu_lambda = np.log(np.sqrt(CI_MIN_FREQ * CI_MAX_FREQ))\n",
    "    sigma_lambda = (np.log(CI_MAX_FREQ) - np.log(CI_MIN_FREQ)) / (2.0 * Z_90)\n",
    "    lambda_rate = pm.Lognormal(\"lambda_rate\", mu=mu_lambda, sigma=sigma_lambda)\n",
    "\n",
    "    success_probs = pm.Beta(\"success_probs\", alpha=alphas, beta=betas, shape=len(MITRE_STAGES))\n",
    "\n",
    "    # Option B: Observed incidents over exposure window\n",
    "    if observed_total_incidents is not None and observed_years is not None:\n",
    "        pm.Poisson(\"obs_incidents\", mu=lambda_rate * observed_years, observed=observed_total_incidents)\n",
    "        print(f\"Conditioning on observed data: {observed_total_incidents} incidents over {observed_years} years.\")\n",
    "    else:\n",
    "        print(\"No observed incident data provided ‚Äî running prior-driven.\")\n",
    "\n",
    "    trace = pm.sample(draws=N_SAMPLES, tune=N_TUNE, chains=N_CHAINS,\n",
    "                      target_accept=TARGET_ACCEPT, random_seed=RANDOM_SEED, progressbar=True)\n",
    "\n",
    "posterior = trace.posterior\n",
    "lambda_draws = posterior[\"lambda_rate\"].values.reshape(-1)\n",
    "success_probs_draws = posterior[\"success_probs\"].values.reshape(-1, len(MITRE_STAGES))\n",
    "R = len(lambda_draws)\n",
    "print(f\"Posterior draws: {R}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c813e7",
   "metadata": {},
   "source": [
    "\n",
    "## 4Ô∏è‚É£ Markov Attacker Simulation with Progression Recording\n",
    "\n",
    "This block simulates attacker behavior per posterior draw, following the MITRE ATT&CK sequence. Each attempt can:\n",
    "- **Retry** a stage up to a maximum, with **geometric penalty** to success per retry.\n",
    "- **Trigger detection** on failed tries, with probability growing per retry.\n",
    "- **Fallback** one stage (with a specified probability) after exhausting retries.\n",
    "- Either **fail early** or **complete** all stages (full compromise).\n",
    "\n",
    "We record per-attempt details and **aggregate** across attempts:\n",
    "- Attempts and retries **per stage**\n",
    "- Distribution of **furthest stage reached**\n",
    "- Total **fallbacks** and **detections**\n",
    "- Estimated **per-attempt success rate**\n",
    "\n",
    "> In notebooks, we default to **serial** execution for reliability; you can flip to parallel if supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1212d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "\n",
    "def simulate_one_attempt_record(success_probs_stage,\n",
    "                                rng: random.Random,\n",
    "                                max_retries_per_stage=MAX_RETRIES_PER_STAGE,\n",
    "                                retry_penalty=RETRY_PENALTY,\n",
    "                                fallback_prob=FALLBACK_PROB,\n",
    "                                detect_base=DETECT_BASE,\n",
    "                                detect_increase_per_retry=DETECT_INC_PER_RETRY) -> Dict:\n",
    "    nstages = len(success_probs_stage)\n",
    "    stage_idx = 0\n",
    "    retries_by_stage = [0] * nstages\n",
    "    stages_tried = []\n",
    "    fallback_count = 0\n",
    "    detected = False\n",
    "    furthest = 0\n",
    "    while True:\n",
    "        if stage_idx >= nstages:\n",
    "            furthest = nstages\n",
    "            return {\"success\": True, \"stages_tried\": stages_tried, \"retries_by_stage\": retries_by_stage,\n",
    "                    \"fallback_count\": fallback_count, \"detected\": detected, \"furthest_stage\": furthest}\n",
    "        p_base = float(success_probs_stage[stage_idx])\n",
    "        progressed = False\n",
    "        for k in range(max_retries_per_stage + 1):\n",
    "            stages_tried.append(stage_idx)\n",
    "            p_try = (retry_penalty ** k) * p_base\n",
    "            if rng.random() < p_try:\n",
    "                retries_by_stage[stage_idx] += k\n",
    "                progressed = True\n",
    "                break\n",
    "            detect_prob = min(max(detect_base + detect_increase_per_retry * k, 0.0), 1.0)\n",
    "            if rng.random() < detect_prob:\n",
    "                detected = True\n",
    "                furthest = max(furthest, stage_idx)\n",
    "                return {\"success\": False, \"stages_tried\": stages_tried, \"retries_by_stage\": retries_by_stage,\n",
    "                        \"fallback_count\": fallback_count, \"detected\": detected, \"furthest_stage\": furthest}\n",
    "        if not progressed:\n",
    "            if stage_idx == 0 or rng.random() > fallback_prob:\n",
    "                furthest = max(furthest, stage_idx)\n",
    "                return {\"success\": False, \"stages_tried\": stages_tried, \"retries_by_stage\": retries_by_stage,\n",
    "                        \"fallback_count\": fallback_count, \"detected\": detected, \"furthest_stage\": furthest}\n",
    "            fallback_count += 1\n",
    "            stage_idx -= 1\n",
    "            continue\n",
    "        furthest = max(furthest, stage_idx + 1)\n",
    "        stage_idx += 1\n",
    "\n",
    "def worker_function_record(args):\n",
    "    per_stage, n_sim, seed = args\n",
    "    rng = random.Random(int(seed))\n",
    "    nstages = len(per_stage)\n",
    "    stage_attempts = np.zeros(nstages, dtype=int)\n",
    "    stage_retries = np.zeros(nstages, dtype=int)\n",
    "    furthest_counts = np.zeros(nstages + 1, dtype=int)\n",
    "    successes = 0\n",
    "    total_fallbacks = 0\n",
    "    total_detections = 0\n",
    "    for _ in range(int(n_sim)):\n",
    "        rec = simulate_one_attempt_record(per_stage, rng)\n",
    "        for s in rec[\"stages_tried\"]:\n",
    "            stage_attempts[int(s)] += 1\n",
    "        for i,r in enumerate(rec[\"retries_by_stage\"]):\n",
    "            stage_retries[i] += int(r)\n",
    "        furthest_counts[int(rec[\"furthest_stage\"])] += 1\n",
    "        successes += int(rec[\"success\"])\n",
    "        total_fallbacks += int(rec[\"fallback_count\"])\n",
    "        total_detections += int(rec[\"detected\"])\n",
    "    return {\"n_sim\": int(n_sim), \"successes\": int(successes),\n",
    "            \"stage_attempts\": stage_attempts, \"stage_retries\": stage_retries,\n",
    "            \"furthest_counts\": furthest_counts, \"fallbacks\": int(total_fallbacks),\n",
    "            \"detections\": int(total_detections)}\n",
    "\n",
    "# Per-draw per-attempt success estimation\n",
    "p_success_simulated = np.zeros(R, dtype=float)\n",
    "args_list = [(success_probs_draws[i, :], N_SIM_PER_DRAW, 1000 + i) for i in range(R)]\n",
    "\n",
    "total_stage_attempts = np.zeros(len(MITRE_STAGES), dtype=int)\n",
    "total_stage_retries = np.zeros(len(MITRE_STAGES), dtype=int)\n",
    "total_furthest = np.zeros(len(MITRE_STAGES) + 1, dtype=int)\n",
    "total_successes = 0\n",
    "total_simulated = 0\n",
    "total_fallbacks = 0\n",
    "total_detections = 0\n",
    "\n",
    "if RUN_PARALLEL:\n",
    "    with ProcessPoolExecutor(max_workers=(N_WORKERS or multiprocessing.cpu_count())) as exe:\n",
    "        for i, res in enumerate(exe.map(worker_function_record, args_list)):\n",
    "            p_success_simulated[i] = res[\"successes\"] / float(res[\"n_sim\"])\n",
    "            total_simulated += res[\"n_sim\"]; total_successes += res[\"successes\"]\n",
    "            total_stage_attempts += res[\"stage_attempts\"]; total_stage_retries += res[\"stage_retries\"]\n",
    "            total_furthest += res[\"furthest_counts\"]; total_fallbacks += res[\"fallbacks\"]\n",
    "            total_detections += res[\"detections\"]\n",
    "else:\n",
    "    for i, args in enumerate(args_list):\n",
    "        res = worker_function_record(args)\n",
    "        p_success_simulated[i] = res[\"successes\"] / float(res[\"n_sim\"])\n",
    "        total_simulated += res[\"n_sim\"]; total_successes += res[\"successes\"]\n",
    "        total_stage_attempts += res[\"stage_attempts\"]; total_stage_retries += res[\"stage_retries\"]\n",
    "        total_furthest += res[\"furthest_counts\"]; total_fallbacks += res[\"fallbacks\"]\n",
    "        total_detections += res[\"detections\"]\n",
    "\n",
    "print(\"Per-draw attacker simulations complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bbc68",
   "metadata": {},
   "source": [
    "\n",
    "## 5Ô∏è‚É£ Posterior Predictive Loss Simulation (FAIR)\n",
    "\n",
    "For each posterior draw we simulate **annual losses**:\n",
    "- Draw a successful incident count from **Poisson(Œª √ó p_success)**.\n",
    "- For each incident, sample **severity multipliers** that scale lognormal FAIR category bodies.\n",
    "- For **Regulatory/Legal** and **Reputation/Competitive**, add **zero-inflated Pareto tails** to capture rare, large-impact events.\n",
    "\n",
    "This produces arrays of annual total losses and per-category losses for summary and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb9976",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng_np = np.random.default_rng(RANDOM_SEED + 1)\n",
    "annual_losses = np.zeros(R, dtype=float)\n",
    "incident_counts = np.zeros(R, dtype=int)\n",
    "cat_loss_matrix = np.zeros((R, len(loss_categories)), dtype=float)\n",
    "\n",
    "for i in range(R):\n",
    "    lam_eff = lambda_draws[i] * p_success_simulated[i]\n",
    "    n_succ = rng_np.poisson(lam_eff)\n",
    "    incident_counts[i] = n_succ\n",
    "    if n_succ == 0:\n",
    "        continue\n",
    "\n",
    "    # Incident-level severity multipliers\n",
    "    severities = rng_np.lognormal(mean=0.0, sigma=0.6, size=n_succ)\n",
    "\n",
    "    # Primary categories (always-on)\n",
    "    prod = np.sum(rng_np.lognormal(cat_mu[0], cat_sigma[0], size=n_succ) * severities)\n",
    "    resp = np.sum(rng_np.lognormal(cat_mu[1], cat_sigma[1], size=n_succ) * severities)\n",
    "\n",
    "    # --- Per-incident correlated Pareto tails ---\n",
    "    tail_trigger_prob = 0.10\n",
    "    reg = 0.0\n",
    "    rep = 0.0\n",
    "    for s in range(n_succ):\n",
    "        sev = severities[s]\n",
    "\n",
    "        # Regulatory/Legal tail\n",
    "        if rng_np.random() < tail_trigger_prob:\n",
    "            xm, alpha = pareto_defaults[\"RegulatoryLegal\"][\"xm\"], pareto_defaults[\"RegulatoryLegal\"][\"alpha\"]\n",
    "            reg += (xm * (1.0 + rng_np.pareto(alpha))) * (1.0 + sev)**1.2\n",
    "\n",
    "        # Reputation/Competitive tail\n",
    "        if rng_np.random() < tail_trigger_prob:\n",
    "            xm, alpha = pareto_defaults[\"ReputationCompetitive\"][\"xm\"], pareto_defaults[\"ReputationCompetitive\"][\"alpha\"]\n",
    "            rep += (xm * (1.0 + rng_np.pareto(alpha))) * (1.0 + sev)**1.2\n",
    "\n",
    "    # Aggregate\n",
    "    cat_loss_matrix[i, :] = [prod, resp, reg, rep]\n",
    "    annual_losses[i] = prod + resp + reg + rep\n",
    "\n",
    "print(\"Posterior predictive losses computed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e2f55",
   "metadata": {},
   "source": [
    "\n",
    "## 6Ô∏è‚É£ Summary Metrics & Progression Analysis\n",
    "\n",
    "We compile high-level KPIs and attacker behavior summaries:\n",
    "- **AAL** mean/median and **95% credible interval** for total annual loss\n",
    "- **Category-level** credible intervals and **share of median AAL**\n",
    "- **Attacker progression** metrics: per-attempt success, fallbacks per attempt, detection rate\n",
    "- Intermediate arrays maintained for downstream plots and CSV export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7c2bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_AAL = annual_losses.mean()\n",
    "median_AAL = np.median(annual_losses)\n",
    "p2_5, p97_5 = np.percentile(annual_losses, [2.5, 97.5])\n",
    "mean_incidents = incident_counts.mean()\n",
    "pct_zero = 100.0 * np.mean(annual_losses == 0.0)\n",
    "\n",
    "print(\"\\nAAL posterior predictive summary:\")\n",
    "print(f\"Mean AAL: ${mean_AAL:,.0f}\")\n",
    "print(f\"Median AAL: ${median_AAL:,.0f}\")\n",
    "print(f\"AAL 95% credible interval (annualized total loss): ${p2_5:,.0f} ‚Äì ${p97_5:,.0f}\")\n",
    "print(f\"Mean successful incidents / year: {mean_incidents:.2f}\")\n",
    "print(f\"% years with zero successful incidents: {pct_zero:.1f}%\\n\")\n",
    "\n",
    "print(\"Category-level annual loss 95% credible intervals:\")\n",
    "for c, cat in enumerate(loss_categories):\n",
    "    low, med, high = np.percentile(cat_loss_matrix[:, c], [2.5, 50, 97.5])\n",
    "    share_med = 100.0 * med / (median_AAL + 1e-12)\n",
    "    print(f\"  {cat:<24s} ${low:,.0f} ‚Äì ${high:,.0f}  (median ${med:,.0f}, ‚âà{share_med:.1f}% of median AAL)\")\n",
    "\n",
    "# Attacker progression aggregates\n",
    "overall_success_rate = total_successes / total_simulated\n",
    "attempts_per_stage = total_stage_attempts / total_simulated\n",
    "avg_retries_per_stage = total_stage_retries / (total_stage_attempts + 1e-12)\n",
    "furthest_stage_prob = total_furthest / total_simulated\n",
    "fallbacks_per_attempt = total_fallbacks / total_simulated\n",
    "detection_rate = total_detections / total_simulated\n",
    "\n",
    "print(\"\\nAttacker progression summary:\")\n",
    "print(f\"Overall per-attempt success: {overall_success_rate:.4f}\")\n",
    "print(f\"Avg fallbacks per attempt: {fallbacks_per_attempt:.4f}\")\n",
    "print(f\"Detection rate: {detection_rate:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbb7268",
   "metadata": {},
   "source": [
    "\n",
    "## 7Ô∏è‚É£ Visualization ‚Äî Long-tail-aware 2√ó2 Dashboard + Log-scale Tail\n",
    "\n",
    "This creates a **2√ó2 dashboard** showing:\n",
    "1. Posterior of **Œª (attacks/year)**\n",
    "2. Posterior of **per-attempt success probability**\n",
    "3. Posterior predictive **successful incidents/year**\n",
    "4. Posterior predictive **annual loss** (clipped to central mass for readability)\n",
    "\n",
    "Then, we add a **log-scale tail plot** for annual loss to inspect heavy tails.  \n",
    "Percentile lines (P50/P90/P95/P99) are drawn to anchor interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2090d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def auto_clip(data, low=0.001, high=0.999):\n",
    "    if len(data) == 0: return data\n",
    "    low_v, high_v = np.percentile(data, [low*100, high*100])\n",
    "    return data[(data >= low_v) & (data <= high_v)]\n",
    "\n",
    "def annotate_percentiles(ax, data, percentiles=(50,90,95,99), scale=1.0, money=True):\n",
    "    if len(data) == 0: return\n",
    "    ymax = ax.get_ylim()[1] if ax.get_ylim()[1] > 0 else 1.0\n",
    "    for p in percentiles:\n",
    "        val = np.percentile(data, p)/scale\n",
    "        ax.axvline(val, color='k', linestyle='--', lw=0.8, alpha=0.85)\n",
    "        label = f\"P{p}=\" + (f\"${val:,.0f}\" if money else f\"{val:,.3f}\")\n",
    "        ax.text(val, ymax*0.92, label, rotation=90, va='top', ha='center', fontsize=8, backgroundcolor='white')\n",
    "\n",
    "scale = 1e6 if PLOT_IN_MILLIONS else 1.0\n",
    "scale_label = \"Million USD\" if PLOT_IN_MILLIONS else \"USD\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# (1) Posterior Œª\n",
    "data_lambda = auto_clip(lambda_draws)\n",
    "axes[0, 0].hist(data_lambda, bins=40, color='steelblue', alpha=0.85)\n",
    "axes[0, 0].set_title(\"Posterior Œª (attacks/year)\")\n",
    "axes[0, 0].set_xlabel(\"Œª (attacks/year)\"); axes[0, 0].set_ylabel(\"Frequency\")\n",
    "annotate_percentiles(axes[0, 0], data_lambda, money=False)\n",
    "\n",
    "# (2) Per-attempt success probability (simulated)\n",
    "data_success = auto_clip(p_success_simulated)\n",
    "axes[0, 1].hist(data_success, bins=40, color='steelblue', alpha=0.85)\n",
    "axes[0, 1].set_title(\"Per-attempt success probability (simulated)\")\n",
    "axes[0, 1].set_xlabel(\"Probability\"); axes[0, 1].set_ylabel(\"Frequency\")\n",
    "annotate_percentiles(axes[0, 1], data_success, money=False)\n",
    "\n",
    "# (3) Successful incidents/year\n",
    "data_incidents = auto_clip(incident_counts)\n",
    "axes[1, 0].hist(data_incidents, bins=40, color='steelblue', alpha=0.85)\n",
    "axes[1, 0].set_title(\"Successful incidents / year (posterior predictive)\")\n",
    "axes[1, 0].set_xlabel(\"Count\"); axes[1, 0].set_ylabel(\"Frequency\")\n",
    "annotate_percentiles(axes[1, 0], data_incidents, money=False)\n",
    "\n",
    "# (4) Annual loss (USD)\n",
    "nonzero = annual_losses[annual_losses > 0]\n",
    "if len(nonzero) > 0:\n",
    "    data_loss = auto_clip(nonzero)\n",
    "    axes[1, 1].hist(data_loss / scale, bins=80, color='steelblue', alpha=0.85)\n",
    "    axes[1, 1].set_title(\"Annual Loss (posterior predictive)\")\n",
    "    axes[1, 1].set_xlabel(f\"Annual Loss ({scale_label})\"); axes[1, 1].set_ylabel(\"Frequency\")\n",
    "    annotate_percentiles(axes[1, 1], data_loss, scale=scale, money=True)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, \"All draws are zero\", ha=\"center\", va=\"center\")\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Log-scale heavy-tail view\n",
    "if len(nonzero) > 0:\n",
    "    low_p, high_p = np.percentile(nonzero, [0.1, 99.9])\n",
    "    filtered = nonzero[(nonzero >= low_p) & (nonzero <= high_p)]\n",
    "    filtered = filtered if len(filtered) >= 10 else nonzero\n",
    "    bins = np.logspace(np.log10(filtered.min()/scale), np.log10(filtered.max()/scale), 100)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.hist(filtered/scale, bins=bins, color='tomato', alpha=0.8)\n",
    "    plt.xscale('log')\n",
    "    plt.title('Annual Loss ‚Äî Log scale (central body shown)')\n",
    "    plt.xlabel(f'Annual Loss ({scale_label})'); plt.ylabel('Frequency')\n",
    "    annotate_percentiles(plt.gca(), filtered, scale=scale, money=True)\n",
    "    plt.grid(True, which='both', ls='--', alpha=0.35)\n",
    "    plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dceb24db",
   "metadata": {},
   "source": [
    "\n",
    "## 8Ô∏è‚É£ Attacker Progression Visualization\n",
    "\n",
    "We translate the aggregated attacker metrics into interpretable charts:\n",
    "- **Stage reach probability** (chance an attacker got to at least each stage)\n",
    "- **Attempts per stage** (log-scaled to accommodate high variance)\n",
    "- **Average retries per attempt** (conditional on a stage being attempted)\n",
    "- **Furthest-stage distribution**, with the last bar labeled **Full Compromise**\n",
    "\n",
    "These views help identify the **choke points** and stages where **defenses or detections** are most effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18402bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "nstages = len(MITRE_STAGES)\n",
    "\n",
    "# Probability attacker reached at least each stage\n",
    "furthest_stage_prob = total_furthest / total_simulated\n",
    "reached_at_least = np.array([np.sum(furthest_stage_prob[k+1:]) for k in range(nstages)])\n",
    "\n",
    "plt.figure(figsize=(12, 3))\n",
    "sns.barplot(x=MITRE_STAGES, y=reached_at_least)\n",
    "plt.xticks(rotation=45, ha='right'); plt.ylabel('Probability reached ‚â• stage')\n",
    "plt.title('Stage reach probability'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Attempts per stage (log scale)\n",
    "attempts_per_stage = total_stage_attempts / total_simulated\n",
    "plt.figure(figsize=(12, 3)); sns.barplot(x=MITRE_STAGES, y=attempts_per_stage)\n",
    "plt.yscale('log'); plt.xticks(rotation=45, ha='right'); plt.ylabel('Avg attempts per simulated attempt (log)')\n",
    "plt.title('Attempts per stage'); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Avg retries per attempt (conditional on attempt)\n",
    "avg_retries_per_stage = total_stage_retries / (total_stage_attempts + 1e-12)\n",
    "plt.figure(figsize=(12, 3)); sns.barplot(x=MITRE_STAGES, y=avg_retries_per_stage)\n",
    "plt.xticks(rotation=45, ha='right'); plt.ylabel('Avg retries per attempt'); plt.title('Retries per stage')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Furthest stage distribution\n",
    "labels = [f\"Reached_{i}\" for i in range(nstages)] + [\"Full Compromise\"]\n",
    "plt.figure(figsize=(12, 3)); sns.barplot(x=labels, y=furthest_stage_prob)\n",
    "plt.xticks(rotation=45, ha='right'); plt.ylabel('Probability'); plt.title('Furthest stage reached')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7023f5a",
   "metadata": {},
   "source": [
    "\n",
    "## 9Ô∏è‚É£ CSV Export for Reuse / Reporting\n",
    "\n",
    "We export three CSVs in the notebook's working directory with a timestamp suffix:\n",
    "1. **Detailed results** ‚Äî one row per posterior draw, with Œª, per-attempt success, incidents, total annual loss, and per-category losses.\n",
    "2. **Summary statistics** ‚Äî mean/median/95% CI by category, plus share of median AAL.\n",
    "3. **Attacker progression stats** ‚Äî stage reach probabilities, attempts per stage, and retries per attempt.\n",
    "\n",
    "These files can be pulled into BI tools, spreadsheets, or versioned for audit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2addd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "script_dir = os.getcwd()\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Detailed per-draw results\n",
    "df = pd.DataFrame({\n",
    "    \"lambda_draw\": lambda_draws,\n",
    "    \"p_success\": p_success_simulated,\n",
    "    \"incidents\": incident_counts,\n",
    "    \"annual_loss\": annual_losses,\n",
    "    **{cat: cat_loss_matrix[:, i] for i, cat in enumerate(loss_categories)}\n",
    "})\n",
    "res_name = os.path.join(script_dir, f\"cyber_risk_simulation_results_{timestamp}.csv\")\n",
    "df.to_csv(res_name, index=False)\n",
    "\n",
    "# Summary stats\n",
    "summary_rows = [{\n",
    "    \"Category\": \"Total Annual Loss\",\n",
    "    \"Mean\": float(np.mean(annual_losses)),\n",
    "    \"Median\": float(np.median(annual_losses)),\n",
    "    \"CI_2.5%\": float(np.percentile(annual_losses, 2.5)),\n",
    "    \"CI_97.5%\": float(np.percentile(annual_losses, 97.5)),\n",
    "    \"Median Share of AAL (%)\": 100.0\n",
    "}]\n",
    "median_AAL_val = float(np.median(annual_losses))\n",
    "for i, cat in enumerate(loss_categories):\n",
    "    vals = cat_loss_matrix[:, i]\n",
    "    summary_rows.append({\n",
    "        \"Category\": cat,\n",
    "        \"Mean\": float(np.mean(vals)),\n",
    "        \"Median\": float(np.median(vals)),\n",
    "        \"CI_2.5%\": float(np.percentile(vals, 2.5)),\n",
    "        \"CI_97.5%\": float(np.percentile(vals, 97.5)),\n",
    "        \"Median Share of AAL (%)\": float(100.0 * (np.median(vals) / (median_AAL_val + 1e-12)))\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "sum_name = os.path.join(script_dir, f\"cyber_risk_simulation_summary_{timestamp}.csv\")\n",
    "summary_df.to_csv(sum_name, index=False)\n",
    "\n",
    "# Attacker progression stats\n",
    "attacker_rows = []\n",
    "for i, stage in enumerate(MITRE_STAGES):\n",
    "    attacker_rows.append({\n",
    "        \"Stage\": stage,\n",
    "        \"Reach_Prob\": float(reached_at_least[i]),\n",
    "        \"Avg_Attempts_per_sim\": float(attempts_per_stage[i]),\n",
    "        \"Avg_Retries_per_attempt\": float(avg_retries_per_stage[i])\n",
    "    })\n",
    "attacker_df = pd.DataFrame(attacker_rows)\n",
    "attacker_name = os.path.join(script_dir, f\"cyber_risk_simulation_attacker_stats_{timestamp}.csv\")\n",
    "attacker_df.to_csv(attacker_name, index=False)\n",
    "\n",
    "print(\"CSV exports written to:\", script_dir)\n",
    "print(\" -\", res_name)\n",
    "print(\" -\", sum_name)\n",
    "print(\" -\", attacker_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36796621",
   "metadata": {},
   "source": [
    "\n",
    "¬© 2025 ‚Äî FAIR‚ÄìMITRE ATT&CK Quantitative Cyber Risk Framework\n",
    "\n",
    "Author: Joshua M. Connors\n",
    "\n",
    "License: Open Use for Non-Commercial Research and Risk Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
